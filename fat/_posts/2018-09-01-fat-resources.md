---
layout: post
title:  Relevant FAT resources
categories:
tags: fairness accountability transparency
author: Kacper
---

* content
{:toc}

Some online resources relevant to the topics of fairness, accountability and transparency in machine learning.<!--more-->

- UC Berkeley [CS294](https://fairmlclass.github.io/): Fairness in Machine Learning course.
- NIPS 2017 tutorial on fairness in ML: [slides](http://fairml.how/tutorial/#/) and [video](https://vimeo.com/248490141).
- [Attacking discrimination with smarter machine learning](https://research.google.com/bigpicture/attacking-discrimination-in-ml/).
- CMU [ece734](http://www.archive.ece.cmu.edu/~ece734/schedule.html) course -- [this](https://www.ece.cmu.edu/~ece734/fall2014/lectures/16.Fairness.pdf) slides in particular.
- Fairness by privacy is not possible: [It's Not Privacy, and It's Not Fair](https://www.stanfordlawreview.org/online/privacy-and-big-data-its-not-privacy-and-its-not-fair/).
- There's a FAT\* 2018 tutorial on [algorithmic fairness](https://algofairness.github.io/fatconference-2018-auditing-tutorial/) (associate code is available here: [BlackBoxAuditing](https://github.com/algofairness/BlackBoxAuditing)) -- see [this]({% post_url /fat/2018-09-02-fat-software %}#fairness) blog post.
- NorthPoint vs. ProPublica [article](https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/) about the COMPAS:
    * "As Gummadi points out, ProPublica compared **false positive rates** and **false negative rates** for blacks and whites and found them to be skewed in favor of whites. Northpointe, in contrast, compared the **PPVs** for different races and found them to be similar. In part because the recidivism rates for blacks and whites do in fact differ, it is mathematically likely that the positive predictive values for people in each group will be similar while the rates of false negatives are not."
